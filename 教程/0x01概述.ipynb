{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b446268c",
   "metadata": {},
   "source": [
    "# 概述\n",
    "从编程范式上说，飞桨支持**声明式编程**和**命令式编程**，即动态图和静态图。\n",
    "- **静态图模式**：先编译后执行的方式。用户需预先定义完整的网络结构，再对网络结构进行编译优化后，才能执行获得计算结果。\n",
    "- **动态图模式**：解析式的执行方式。用户无需预先定义完整的网络结构，每写一行网络代码，即可同时获得计算结果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfd1bbf",
   "metadata": {},
   "source": [
    "## 动态图模式\n",
    "目前飞桨默认的模式是动态图，如果想启用或关闭静态图，可以调用\n",
    "- `paddle.enable_static()`\n",
    "- `paddle.disable_static()`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcbb2752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在动态模式下，调用paddle.to_tensor()之后，\n",
      "x = Tensor(shape=[2, 2], dtype=float32, place=CUDAPlace(0), stop_gradient=True,\n",
      "       [[1., 1.],\n",
      "        [1., 1.]])\n",
      "------------------------------\n",
      "在动态模式下，进行计算之后，\n",
      "x = [[11. 11.]\n",
      " [11. 11.]]\n"
     ]
    }
   ],
   "source": [
    "# 以 x += 10为例，比较动态图和静态图的区别\n",
    "import paddle\n",
    "import numpy as np\n",
    "\n",
    "# 创建值为1的[2, 2]二维数组\n",
    "data = np.ones([2, 2], np.float32)\n",
    "\n",
    "# 默认动态图模式下，将数据转化为Tensor类型\n",
    "x = paddle.to_tensor(data)\n",
    "\n",
    "print(\"在动态模式下，调用paddle.to_tensor()之后，\")\n",
    "print(\"x =\", x)\n",
    "\n",
    "x += 10\n",
    "print('------------------------------')\n",
    "print(\"在动态模式下，进行计算之后，\", )\n",
    "print(\"x =\", x.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ef7bc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "静态图模式下，调用paddle.static.data接口之后，\n",
      "x = var x : LOD_TENSOR.shape(2, 2).dtype(float32).stop_gradient(True)\n",
      "静态图模式下，运行之后的数据： [array([[11., 11.],\n",
      "       [11., 11.]], dtype=float32)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\lc-or\\lib\\site-packages\\paddle\\fluid\\executor.py:1279: UserWarning: There are no operators in the program to be executed. If you pass Program manually, please use fluid.program_guard to ensure the current Program is being used.\n",
      "  warnings.warn(error_info)\n"
     ]
    }
   ],
   "source": [
    "# 启动静态图模式\n",
    "paddle.enable_static()\n",
    "\n",
    "# 添加Paddle对计算图的静态描述 -- Program\n",
    "main_program = paddle.static.Program() \n",
    "startup_program = paddle.static.Program()\n",
    "\n",
    "with paddle.static.program_guard(main_program=main_program, startup_program=startup_program):\n",
    "    x = paddle.static.data(name='x', shape=[2, 2], dtype='float32')\n",
    "    print(\"静态图模式下，调用paddle.static.data接口之后，\")\n",
    "    print(\"x =\", x)\n",
    "    # 静态图模式下，对占位符Variable类型的数据执行操作, 并且需要用户指定运行的设备\n",
    "    x += 10\n",
    "    place = paddle.CPUPlace()\n",
    "    # 创建执行器来运行组网的Program，并用place指定在什么设备上运行\n",
    "    exe = paddle.static.Executor(place=place)\n",
    "    # 进行初始化操作\n",
    "    exe.run(startup_program)\n",
    "    # 使用执行器执行已经记录的所有操作\n",
    "    # 可以通过fetch_list参数来指定要获取哪些变量的计算结果\n",
    "    # 也可以通过feed参数来传入数据\n",
    "    data_after_run = exe.run(fetch_list=[x], feed={'x': data})\n",
    "    print(\"静态图模式下，运行之后的数据：\", data_after_run)\n",
    "    \n",
    "# 关闭静态图模式，回归动态图模式\n",
    "paddle.disable_static()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4ee857",
   "metadata": {},
   "source": [
    "- 一个Program集合通常包含启动程序(startup_program)与主程序(main_program)，启动程序用来初始化参数，主程序用来包含网络结构和参数\n",
    "- paddle.static.program_guard()接口配合with语句将with block里面所有的算子和变量添加进全局主程序和启动程序。 paddle.static.data()会在全局block种创建Tensor变量，可以被计算图中的算子访问，也可以作为占位符用于数据输入。\n",
    "- 从结果可以看出：\n",
    "    - 动态图模式下，所有的操作在运行时就已经完成\n",
    "    - 静态图模式下，过程中并没有实际执行操作，上述例子中可以看到只能打印声明的类型\n",
    "- 飞桨静态图专用API请参考[paddle.static](https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/static/Overview_cn.html)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db2a50b",
   "metadata": {},
   "source": [
    "### 1. 动态图模型训练\n",
    "1. 定义数据读取器：读取数据预处理操作\n",
    "2. 定义模型和优化器：搭建神经网络结构\n",
    "3. 训练：配置优化器，学习率，训练参数。\n",
    "4. 评估测试\n",
    "5. 模型保存和加载的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee8ae846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数据读取器(调用封装好的数据集API)\n",
    "import numpy as np\n",
    "import paddle\n",
    "\n",
    "# 定义batch_size\n",
    "BATCH_SIZE = 64\n",
    "# 调用DataLoader将输入数据打包成BATCH_SIZE大小的批处理数据\n",
    "class MnistDataset(paddle.vision.datasets.MNIST):\n",
    "    def __init__(self, mode, return_label=True):\n",
    "        super(MnistDataset, self).__init__(mode=mode)\n",
    "        self.return_label = return_label\n",
    "    \n",
    "    # reshape 并将[0, 255] 转化为 [-1, 1]\n",
    "    def __getitem__(self, idx):\n",
    "        img = np.reshape(self.images[idx], [1, 28, 28])\n",
    "        img = img / 255.0 * 2.0 - 1.0\n",
    "        if self.return_label:\n",
    "            return img, np.array(self.labels[idx].astype('int64'))\n",
    "        return img,\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "train_reader = paddle.io.DataLoader(MnistDataset(mode='train'), batch_size=BATCH_SIZE, drop_last=True)\n",
    "test_reader = paddle.io.DataLoader(MnistDataset(mode='test'), batch_size=BATCH_SIZE, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfe5f5f",
   "metadata": {},
   "source": [
    "- 在动态图模式中，参数和变量的存储管理方式与静态图不同。\n",
    "    - 动态图模式下，网络中学习的参数和中间变量，**生命周期和 Python 对象的生命周期是一致的**。简单来说，一个 Python 对象的生命周期结束，相应的存储空间就会释放。  \n",
    "- 对于一个网络模型，在模型学习的过程中参数会不断更新，所以参数需要在整个学习周期内一直保持存在，因此需要一个机制保持网络的参数不被释放。\n",
    "    - 飞桨动态图模式采用了继承自paddle.nn.Layer的面向对象设计的方法管理所有参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02af0359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义网络结构和优化器\n",
    "import paddle\n",
    "from paddle.nn import Conv2D, MaxPool2D, ReLU\n",
    "\n",
    "# 定义网络必须继承自paddle.nn.Layer\n",
    "class SimpleImgConvPool(paddle.nn.Layer):\n",
    "    # 在__init__构造函数中会执行变量的初始化、参数初始化、子网络初始化的操作\n",
    "    # 本例中执行了Conv2D和MaxPool2D网络的初始化操作\n",
    "    def __init__(self,\n",
    "                 in_channels, out_channels, filter_size, pool_size, pool_stride,\n",
    "                 pool_padding=0, conv_stride=1, conv_padding=0, conv_dilation=1,\n",
    "                 conv_groups=1, weight_attr=None, bias_attr=None):\n",
    "        super(SimpleImgConvPool, self).__init__()\n",
    "\n",
    "        # Conv2D网络的初始化\n",
    "        self._conv2d = Conv2D(in_channels=in_channels,out_channels=out_channels,\n",
    "                            kernel_size=filter_size, stride=conv_stride,\n",
    "                            padding=conv_padding, dilation=conv_dilation,\n",
    "                            groups=conv_groups, weight_attr=weight_attr,\n",
    "                            bias_attr=bias_attr)\n",
    "        # ReLU激活的初始化\n",
    "        self._relu = ReLU()\n",
    "\n",
    "        # Pool2D网络的初始化\n",
    "        self._pool2d = MaxPool2D(kernel_size=pool_size, stride=pool_stride, padding=pool_padding)\n",
    "\n",
    "    # forward函数实现了SimpleImgConvPool网络的执行逻辑\n",
    "    def forward(self, inputs):\n",
    "        x = self._conv2d(inputs)\n",
    "        x = self._relu(x)\n",
    "        x = self._pool2d(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c16ef636",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(MNIST, self).__init__()\n",
    "        self._simple_img_conv_pool_1 = SimpleImgConvPool(\n",
    "            1, 20, 5, 2, 2)\n",
    "        self._simple_img_conv_pool_2 = SimpleImgConvPool(\n",
    "            20, 50, 5, 2, 2)\n",
    "        \n",
    "        # self.pool_2_shape变量定义了经过self._simple_img_conv_pool_2层之后的数据\n",
    "        # 除了batch_size维度之外其他维度的乘积\n",
    "        self.pool_2_shape = 50 * 4 * 4\n",
    "        # self.pool_2_shape、self.size定义了self.output_weight参数的维度\n",
    "        self.size = 10\n",
    "        # 定义全连接层的参数\n",
    "        self.output_weight = self.create_parameter([self.pool_2_shape, self.size])\n",
    "\n",
    "        # 定义计算accuracy的层\n",
    "        self.accuracy = paddle.metric.Accuracy()\n",
    "    \n",
    "    # forward函数实现了MNIST网络的执行逻辑\n",
    "    def forward(self, inputs, label=None):\n",
    "        x = self._simple_img_conv_pool_1(inputs)\n",
    "        x = self._simple_img_conv_pool_2(x)\n",
    "        x = paddle.reshape(x, shape=[-1, self.pool_2_shape])\n",
    "        x = paddle.matmul(x, self.output_weight)\n",
    "        x = paddle.nn.functional.softmax(x)\n",
    "        if label is not None:\n",
    "            # Reset只返回当前batch的准确率\n",
    "            self.accuracy.reset()\n",
    "            correct = self.accuracy.compute(x, label)\n",
    "            self.accuracy.update(correct)\n",
    "            acc = self.accuracy.accumulate()\n",
    "            return x, acc\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e257b602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 0, Loss = [2.3319526], Accuracy = 0.125\n",
      "Epoch 0 step 100, Loss = [1.853616], Accuracy = 0.609375\n",
      "Epoch 0 step 200, Loss = [1.6881948], Accuracy = 0.765625\n",
      "Epoch 0 step 300, Loss = [1.6814728], Accuracy = 0.78125\n",
      "Epoch 0 step 400, Loss = [1.589998], Accuracy = 0.875\n",
      "Epoch 0 step 500, Loss = [1.6732543], Accuracy = 0.8125\n",
      "Epoch 0 step 600, Loss = [1.6490974], Accuracy = 0.8125\n",
      "Epoch 0 step 700, Loss = [1.8125134], Accuracy = 0.640625\n",
      "Epoch 0 step 800, Loss = [1.6831717], Accuracy = 0.78125\n",
      "Epoch 0 step 900, Loss = [1.6284146], Accuracy = 0.828125\n",
      "Epoch 1 step 0, Loss = [1.5831273], Accuracy = 0.875\n",
      "Epoch 1 step 100, Loss = [1.6316577], Accuracy = 0.828125\n",
      "Epoch 1 step 200, Loss = [1.5331774], Accuracy = 0.9375\n",
      "Epoch 1 step 300, Loss = [1.5997106], Accuracy = 0.859375\n",
      "Epoch 1 step 400, Loss = [1.5070084], Accuracy = 0.953125\n",
      "Epoch 1 step 500, Loss = [1.5377495], Accuracy = 0.921875\n",
      "Epoch 1 step 600, Loss = [1.5861411], Accuracy = 0.875\n",
      "Epoch 1 step 700, Loss = [1.6176636], Accuracy = 0.84375\n",
      "Epoch 1 step 800, Loss = [1.584316], Accuracy = 0.875\n",
      "Epoch 1 step 900, Loss = [1.5924151], Accuracy = 0.875\n",
      "Epoch 2 step 0, Loss = [1.5682673], Accuracy = 0.890625\n",
      "Epoch 2 step 100, Loss = [1.6255662], Accuracy = 0.828125\n",
      "Epoch 2 step 200, Loss = [1.5187565], Accuracy = 0.9375\n",
      "Epoch 2 step 300, Loss = [1.6121665], Accuracy = 0.84375\n",
      "Epoch 2 step 400, Loss = [1.5082595], Accuracy = 0.953125\n",
      "Epoch 2 step 500, Loss = [1.5545291], Accuracy = 0.90625\n",
      "Epoch 2 step 600, Loss = [1.5837486], Accuracy = 0.875\n",
      "Epoch 2 step 700, Loss = [1.6169242], Accuracy = 0.84375\n",
      "Epoch 2 step 800, Loss = [1.5828198], Accuracy = 0.875\n",
      "Epoch 2 step 900, Loss = [1.4861892], Accuracy = 0.96875\n",
      "Epoch 3 step 0, Loss = [1.462179], Accuracy = 1.0\n",
      "Epoch 3 step 100, Loss = [1.4771385], Accuracy = 0.984375\n",
      "Epoch 3 step 200, Loss = [1.5011755], Accuracy = 0.953125\n",
      "Epoch 3 step 300, Loss = [1.4769815], Accuracy = 0.984375\n",
      "Epoch 3 step 400, Loss = [1.4852521], Accuracy = 0.96875\n",
      "Epoch 3 step 500, Loss = [1.4946961], Accuracy = 0.96875\n",
      "Epoch 3 step 600, Loss = [1.4767733], Accuracy = 0.984375\n",
      "Epoch 3 step 700, Loss = [1.484043], Accuracy = 0.984375\n",
      "Epoch 3 step 800, Loss = [1.4840667], Accuracy = 0.984375\n",
      "Epoch 3 step 900, Loss = [1.4811192], Accuracy = 0.984375\n",
      "Epoch 4 step 0, Loss = [1.4630738], Accuracy = 1.0\n",
      "Epoch 4 step 100, Loss = [1.4769131], Accuracy = 0.984375\n",
      "Epoch 4 step 200, Loss = [1.4821001], Accuracy = 0.984375\n",
      "Epoch 4 step 300, Loss = [1.479748], Accuracy = 0.984375\n",
      "Epoch 4 step 400, Loss = [1.4725564], Accuracy = 0.984375\n",
      "Epoch 4 step 500, Loss = [1.4917805], Accuracy = 0.96875\n",
      "Epoch 4 step 600, Loss = [1.4767965], Accuracy = 0.984375\n",
      "Epoch 4 step 700, Loss = [1.5224366], Accuracy = 0.9375\n",
      "Epoch 4 step 800, Loss = [1.4889084], Accuracy = 0.96875\n",
      "Epoch 4 step 900, Loss = [1.4771459], Accuracy = 0.984375\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from paddle.optimizer import Adam\n",
    "\n",
    "\n",
    "# 定义MNIST类的对象\n",
    "mnist = MNIST()\n",
    "# 定义优化器为Adam，学习率learning_rate为0.001\n",
    "# 注意动态图模式下必须传入parameters参数，该参数为需要优化的网络参数，本例需要优化mnist网络中的所有参数\n",
    "adam = Adam(learning_rate=0.001, parameters=mnist.parameters())\n",
    "\n",
    "# 设置全部样本的训练次数\n",
    "epoch_num = 5\n",
    "    \n",
    "# 执行epoch_num次训练\n",
    "for epoch in range(epoch_num):\n",
    "    # 读取训练数据进行训练\n",
    "    for batch_id, data in enumerate(train_reader()):\n",
    "        # train_reader 返回的是img和label已经是Tensor类型，可以动态图使用\n",
    "        img = data[0]\n",
    "        label = data[1]\n",
    "        \n",
    "        # 网络正向执行\n",
    "        pred, acc = mnist(img, label)\n",
    "            \n",
    "        # 计算损失值\n",
    "        loss = paddle.nn.functional.cross_entropy(pred, label)\n",
    "        avg_loss = paddle.mean(loss)\n",
    "        # 执行反向计算\n",
    "        avg_loss.backward()\n",
    "        # 参数更新\n",
    "        adam.step()\n",
    "        # 将本次计算的梯度值清零，以便进行下一次迭代和梯度更新\n",
    "        adam.clear_grad()\n",
    "            \n",
    "        # 输出对应epoch、batch_id下的损失值，预测精确度\n",
    "        if batch_id % 100 == 0:\n",
    "            print(\"Epoch {} step {}, Loss = {:}, Accuracy = {:}\".format(\n",
    "                    epoch, batch_id, avg_loss.numpy(), acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58f25eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存训练好的模型\n",
    "model_dict = mnist.state_dict()\n",
    "paddle.save(model_dict, \"mnist.pdparams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d7b54147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint loaded\n",
      "Eval avg_loss is: 1.475463283367646, acc is: 0.9859775641025641\n"
     ]
    }
   ],
   "source": [
    "# 评估测试\n",
    "mnist_eval = MNIST()\n",
    "# 加载保存的模型\n",
    "model_dict = paddle.load(\"mnist.pdparams\")\n",
    "mnist_eval.set_state_dict(model_dict)\n",
    "print(\"checkpoint loaded\")\n",
    "\n",
    "# model.eval()      #切换到评估模式\n",
    "# model.train()     #切换到训练模式\n",
    "# 切换到预测评估模式\n",
    "mnist_eval.eval()\n",
    "\n",
    "acc_set = []\n",
    "avg_loss_set = []\n",
    "# 读取测试数据进行评估测试\n",
    "for batch_id, data in enumerate(test_reader()):\n",
    "    img = data[0]\n",
    "    label = data[1]\n",
    "\n",
    "    # 网络正向执行\n",
    "    prediction, acc = mnist_eval(img, label)\n",
    "        \n",
    "    # 计算损失值\n",
    "    loss = paddle.nn.functional.cross_entropy(prediction, label)\n",
    "    avg_loss = paddle.mean(loss)\n",
    "\n",
    "    acc_set.append(float(acc))\n",
    "    avg_loss_set.append(float(avg_loss.numpy()))\n",
    "        \n",
    "# 输出不同 batch 数据下损失值和准确率的平均值\n",
    "acc_val_mean = np.array(acc_set).mean()\n",
    "avg_loss_val_mean = np.array(avg_loss_set).mean()\n",
    "print(\"Eval avg_loss is: {}, acc is: {}\".format(avg_loss_val_mean, acc_val_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64802350",
   "metadata": {},
   "source": [
    "在动态图模式下，模型和优化器在不同的模块中，所以模型和优化器分别在不同的对象中存储，使得模型参数和优化器信息需分别存储。因此模型的保存需要单独调用模型和优化器中的 state_dict() 接口，同样模型的加载也需要单独进行处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c787899f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 保存模型参数\n",
    "paddle.save(mnist.state_dict(), \"mnist.pdparams\")\n",
    "# 2. 保存优化器信息\n",
    "paddle.save(adam.state_dict(), \"adam.pdopt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "73dea0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 获取模型参数和优化器信息\n",
    "model_state = paddle.load(\"mnist.pdparams\")\n",
    "opt_state = paddle.load(\"adam.pdopt\")\n",
    "# 2. 加载模型参数\n",
    "mnist.set_state_dict(model_state)\n",
    "# 3. 加载优化器信息\n",
    "adam.set_state_dict(opt_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0246a62",
   "metadata": {},
   "source": [
    "### 2. 多卡训练\n",
    "针对数据量、计算量较大的任务，我们需要多卡并行训练，以提高训练效率。目前动态图模式可支持GPU的单机多卡训练方式。  \n",
    "动态图多卡通过 Python 基础库 `subprocess` 在每一张 GPU 上启动单独的 Python 程序的方式，每张卡的程序独立运行，只是在每一轮梯度计算完成之后，所有的程序进行梯度的同步，然后更新训练的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "106f45ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import paddle\n",
    "import paddle.distributed as dist\n",
    "from paddle.optimizer import Adam\n",
    "\n",
    "# 准备多卡环境\n",
    "dist.init_parallel_env()\n",
    "\n",
    "epoch_num = 5\n",
    "BATCH_SIZE = 64\n",
    "mnist = MNIST()\n",
    "adam = Adam(learning_rate=0.001, parameters=mnist.parameters())\n",
    "\n",
    "# 数据并行模块\n",
    "mnist = paddle.DataParallel(mnist)\n",
    "\n",
    "# 通过调用paddle.io.DataLoader来构造reader\n",
    "# 需要使用DistributedBatchSampler为多张卡拆分数据\n",
    "train_sampler = paddle.io.DistributedBatchSampler(MnistDataset(mode='train'),\n",
    "                                                  batch_size=BATCH_SIZE, \n",
    "                                                  drop_last=True)\n",
    "train_reader = paddle.io.DataLoader(MnistDataset(mode='train'), batch_sampler=train_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fc27be2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:14: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:14: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "C:\\Users\\wangjiabin01\\AppData\\Local\\Temp/ipykernel_20668/3844766566.py:14: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if batch_id % 100 == 0 and batch_id is not 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 100, Loss = [1.7339315], Accuracy = 0.71875\n",
      "Epoch 0 step 200, Loss = [1.573685], Accuracy = 0.90625\n",
      "Epoch 0 step 300, Loss = [1.494304], Accuracy = 0.984375\n",
      "Epoch 0 step 400, Loss = [1.4964607], Accuracy = 0.96875\n",
      "Epoch 0 step 500, Loss = [1.4984132], Accuracy = 0.96875\n",
      "Epoch 0 step 600, Loss = [1.4760737], Accuracy = 0.984375\n",
      "Epoch 0 step 700, Loss = [1.5143863], Accuracy = 0.953125\n",
      "Epoch 0 step 800, Loss = [1.4996874], Accuracy = 0.96875\n",
      "Epoch 0 step 900, Loss = [1.506871], Accuracy = 0.953125\n",
      "Epoch 1 step 100, Loss = [1.4795102], Accuracy = 0.984375\n",
      "Epoch 1 step 200, Loss = [1.5102603], Accuracy = 0.953125\n",
      "Epoch 1 step 300, Loss = [1.4787824], Accuracy = 0.984375\n",
      "Epoch 1 step 400, Loss = [1.4697437], Accuracy = 1.0\n",
      "Epoch 1 step 500, Loss = [1.4853926], Accuracy = 0.96875\n",
      "Epoch 1 step 600, Loss = [1.476786], Accuracy = 0.984375\n",
      "Epoch 1 step 700, Loss = [1.4872568], Accuracy = 0.984375\n",
      "Epoch 1 step 800, Loss = [1.4995837], Accuracy = 0.96875\n",
      "Epoch 1 step 900, Loss = [1.4791582], Accuracy = 0.984375\n",
      "Epoch 2 step 100, Loss = [1.4768608], Accuracy = 0.984375\n",
      "Epoch 2 step 200, Loss = [1.5056622], Accuracy = 0.953125\n",
      "Epoch 2 step 300, Loss = [1.4768693], Accuracy = 0.984375\n",
      "Epoch 2 step 400, Loss = [1.4739081], Accuracy = 0.984375\n",
      "Epoch 2 step 500, Loss = [1.4850988], Accuracy = 0.984375\n",
      "Epoch 2 step 600, Loss = [1.4766414], Accuracy = 0.984375\n",
      "Epoch 2 step 700, Loss = [1.477201], Accuracy = 0.984375\n",
      "Epoch 2 step 800, Loss = [1.5023768], Accuracy = 0.953125\n",
      "Epoch 2 step 900, Loss = [1.499489], Accuracy = 0.953125\n",
      "Epoch 3 step 100, Loss = [1.4769902], Accuracy = 0.984375\n",
      "Epoch 3 step 200, Loss = [1.4956194], Accuracy = 0.96875\n",
      "Epoch 3 step 300, Loss = [1.4767957], Accuracy = 0.984375\n",
      "Epoch 3 step 400, Loss = [1.4612896], Accuracy = 1.0\n",
      "Epoch 3 step 500, Loss = [1.4767673], Accuracy = 0.984375\n",
      "Epoch 3 step 600, Loss = [1.4765651], Accuracy = 0.984375\n",
      "Epoch 3 step 700, Loss = [1.4987619], Accuracy = 0.953125\n",
      "Epoch 3 step 800, Loss = [1.4844666], Accuracy = 0.984375\n",
      "Epoch 3 step 900, Loss = [1.5060195], Accuracy = 0.953125\n",
      "Epoch 4 step 100, Loss = [1.4862502], Accuracy = 0.96875\n",
      "Epoch 4 step 200, Loss = [1.4931896], Accuracy = 0.96875\n",
      "Epoch 4 step 300, Loss = [1.4769013], Accuracy = 0.984375\n",
      "Epoch 4 step 400, Loss = [1.4639924], Accuracy = 1.0\n",
      "Epoch 4 step 500, Loss = [1.4792469], Accuracy = 0.984375\n",
      "Epoch 4 step 600, Loss = [1.4767598], Accuracy = 0.984375\n",
      "Epoch 4 step 700, Loss = [1.4949207], Accuracy = 0.96875\n",
      "Epoch 4 step 800, Loss = [1.4935774], Accuracy = 0.96875\n",
      "Epoch 4 step 900, Loss = [1.4774842], Accuracy = 0.984375\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch_num):\n",
    "    for batch_id, data in enumerate(train_reader()):\n",
    "        img = data[0]\n",
    "        label = data[1]\n",
    "        label.stop_gradient = True\n",
    "            \n",
    "        pred, acc = mnist(img, label)\n",
    "            \n",
    "        loss = paddle.nn.functional.cross_entropy(pred, label)\n",
    "        avg_loss = paddle.mean(loss)\n",
    "        avg_loss.backward()\n",
    "        adam.step()\n",
    "        adam.clear_grad()\n",
    "        if batch_id % 100 == 0 and batch_id is not 0:\n",
    "            print(\"Epoch {} step {}, Loss = {:}, Accuracy = {:}\".format(\n",
    "                    epoch, batch_id, avg_loss.numpy(), acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d165df14",
   "metadata": {},
   "source": [
    "飞桨动态图多进程多卡模型训练启动时，需要指定使用的 GPU，比如使用 0,1 卡，可执行如下命令启动训练  \n",
    "`$ python -m paddle.distributed.launch --gpus=0,1 --log_dir ./mylog train.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f356277d",
   "metadata": {},
   "source": [
    "### 3. 模型部署\n",
    "动态图虽然有非常多的优点，但是如果用户希望使用 C++ 部署已经训练好的模型，会存在一些不便利。比如，动态图中可使用 Python 原生的控制流，包含 if/else、switch、for/while，这些控制流需要通过一定的机制才能映射到 C++ 端，实现在 C++ 端的部署。\n",
    "- 如果用户使用的 if/else、switch、for/while 与输入（包括输入的值和 shape ）无关，则可以使用 `@paddle.jit.to_static` 将前向动态图模型转换为静态图模型。可以将动态图保存后做在线C++预测；\n",
    "- 除此以外，用户也可使用转换后的静态图模型在Python端做预测，通常比原先的动态图性能更好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d273a3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\lc-or\\lib\\site-packages\\paddle\\fluid\\layers\\utils.py:77: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  return (isinstance(seq, collections.Sequence) and\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 0, Loss = [2.351206]\n",
      "Epoch 0 step 100, Loss = [1.7277366]\n",
      "Epoch 0 step 200, Loss = [1.6548624]\n",
      "Epoch 0 step 300, Loss = [1.759743]\n",
      "Epoch 0 step 400, Loss = [1.6397439]\n",
      "Epoch 0 step 500, Loss = [1.5950389]\n",
      "Epoch 0 step 600, Loss = [1.5542469]\n",
      "Epoch 0 step 700, Loss = [1.634324]\n",
      "Epoch 0 step 800, Loss = [1.5691906]\n",
      "Epoch 0 step 900, Loss = [1.5881759]\n",
      "Epoch 1 step 0, Loss = [1.6015033]\n",
      "Epoch 1 step 100, Loss = [1.5508146]\n",
      "Epoch 1 step 200, Loss = [1.5696216]\n",
      "Epoch 1 step 300, Loss = [1.5691619]\n",
      "Epoch 1 step 400, Loss = [1.6166158]\n",
      "Epoch 1 step 500, Loss = [1.5700891]\n",
      "Epoch 1 step 600, Loss = [1.5532005]\n",
      "Epoch 1 step 700, Loss = [1.6156965]\n",
      "Epoch 1 step 800, Loss = [1.5692351]\n",
      "Epoch 1 step 900, Loss = [1.5078506]\n",
      "Epoch 2 step 0, Loss = [1.4957088]\n",
      "Epoch 2 step 100, Loss = [1.4897172]\n",
      "Epoch 2 step 200, Loss = [1.4915651]\n",
      "Epoch 2 step 300, Loss = [1.4922736]\n",
      "Epoch 2 step 400, Loss = [1.4618129]\n",
      "Epoch 2 step 500, Loss = [1.491091]\n",
      "Epoch 2 step 600, Loss = [1.4768126]\n",
      "Epoch 2 step 700, Loss = [1.5000019]\n",
      "Epoch 2 step 800, Loss = [1.5245886]\n",
      "Epoch 2 step 900, Loss = [1.483244]\n",
      "Epoch 3 step 0, Loss = [1.4765359]\n",
      "Epoch 3 step 100, Loss = [1.4782643]\n",
      "Epoch 3 step 200, Loss = [1.4757968]\n",
      "Epoch 3 step 300, Loss = [1.4768187]\n",
      "Epoch 3 step 400, Loss = [1.464983]\n",
      "Epoch 3 step 500, Loss = [1.4768367]\n",
      "Epoch 3 step 600, Loss = [1.4762487]\n",
      "Epoch 3 step 700, Loss = [1.4811368]\n",
      "Epoch 3 step 800, Loss = [1.4884627]\n",
      "Epoch 3 step 900, Loss = [1.4829509]\n",
      "Epoch 4 step 0, Loss = [1.4767363]\n",
      "Epoch 4 step 100, Loss = [1.4777266]\n",
      "Epoch 4 step 200, Loss = [1.491503]\n",
      "Epoch 4 step 300, Loss = [1.4793357]\n",
      "Epoch 4 step 400, Loss = [1.4613845]\n",
      "Epoch 4 step 500, Loss = [1.492286]\n",
      "Epoch 4 step 600, Loss = [1.4767517]\n",
      "Epoch 4 step 700, Loss = [1.5086186]\n",
      "Epoch 4 step 800, Loss = [1.4967569]\n",
      "Epoch 4 step 900, Loss = [1.4788144]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import paddle\n",
    "from paddle.optimizer import Adam\n",
    "\n",
    "\n",
    "class MNIST(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(MNIST, self).__init__()\n",
    "        self._simple_img_conv_pool_1 = SimpleImgConvPool(\n",
    "            1, 20, 5, 2, 2)\n",
    "        self._simple_img_conv_pool_2 = SimpleImgConvPool(\n",
    "            20, 50, 5, 2, 2)\n",
    "\n",
    "        self.pool_2_shape = 50 * 4 * 4\n",
    "        self.size = 10\n",
    "        self.output_weight = self.create_parameter(\n",
    "            [self.pool_2_shape, self.size])\n",
    "\n",
    "    @paddle.jit.to_static # 在 forward 函数添加装饰器\n",
    "    def forward(self, inputs):\n",
    "        x = self._simple_img_conv_pool_1(inputs)\n",
    "        x = self._simple_img_conv_pool_2(x)\n",
    "        x = paddle.reshape(x, shape=[-1, self.pool_2_shape])\n",
    "        x = paddle.matmul(x, self.output_weight)\n",
    "        x = paddle.nn.functional.softmax(x)\n",
    "        return x\n",
    "    \n",
    "epoch_num = 5\n",
    "BATCH_SIZE = 64\n",
    "mnist = MNIST()\n",
    "adam = Adam(learning_rate=0.001, parameters=mnist.parameters())\n",
    "for epoch in range(epoch_num):\n",
    "    for batch_id, data in enumerate(train_reader()):\n",
    "        img = data[0]\n",
    "        label = data[1]\n",
    "        pred = mnist(img)\n",
    "\n",
    "        loss = paddle.nn.functional.cross_entropy(pred, label)\n",
    "        avg_loss = paddle.mean(loss)\n",
    "        avg_loss.backward()\n",
    "        adam.step()\n",
    "        adam.clear_grad()\n",
    "\n",
    "        if batch_id % 100 == 0:\n",
    "            print(\"Epoch {} step {}, Loss = {:}\".format(\n",
    "                    epoch, batch_id, avg_loss.numpy()))\n",
    "            \n",
    "# 此处的 path 参数为前缀，而非文件名\n",
    "paddle.jit.save(mnist, \"inference/mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c1986efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load MNIST predict: [[1.5050951e-04 5.3357985e-09 1.9456142e-04 1.8036899e-03 1.4148206e-06\n",
      "  5.3630345e-02 3.0761273e-03 1.5505937e-05 9.2861640e-01 1.2511353e-02]] \n"
     ]
    }
   ],
   "source": [
    "# 通过调用 paddle.jit.save 接口将静态图模型保存为用于预测部署的模型\n",
    "# 之后如果在Python中想要使用，可以利用 paddle.jit.load 接口将保存的模型加载。\n",
    "load_mnist = paddle.jit.load(\"inference/mnist\")\n",
    "\n",
    "load_mnist.eval()\n",
    "x = paddle.randn([1, 1, 28, 28], 'float32')\n",
    "pred = load_mnist(x)\n",
    "\n",
    "print(\"Load MNIST predict: {:} \".format(pred.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad42fbe",
   "metadata": {},
   "source": [
    "## 静态图模式\n",
    "静态图的优势在于在运行时所有的操作和执行顺序都已经定义完成了，能够根据全局信息来做各种优化策略，比如合并相邻操作来进行加速或者减少中间变量，因此对于同样的网络结构，使用静态图模型运行往往能够获取更好的性能和更少的内存占用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4da3c17",
   "metadata": {},
   "source": [
    "### 1. 静态图的数据表示和定义\n",
    "静态图也使用变量和常量来表示数据，但是由于在调用**执行器**之前，静态图并不执行实际操作（这个阶段一般称为“组网阶段”或者“编译阶段”），因此也不会在此时读入数据，所以在静态图中还需要一种特殊的变量来表示输入数据，一般称为“**占位符**”。在飞桨中我们使用 paddle.static.data 来创建占位符，paddle.static.data 需要指定Variable的形状信息和数据类型，当遇到无法确定的维度时，可以将相应维度指定为None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e6a0954d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = var x : LOD_TENSOR.shape(3, -1).dtype(int64).stop_gradient(True)\n",
      "batched_x = var batched_x : LOD_TENSOR.shape(-1, 3, -1).dtype(int64).stop_gradient(True)\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "paddle.enable_static()\n",
    "x = paddle.static.data(name='x', shape=[3, None], dtype=\"int64\")\n",
    "print('x =', x)\n",
    "# 大多数网络都会采用batch方式进行数据组织，batch大小在定义时不确定\n",
    "batched_x = paddle.static.data(name=\"batched_x\", shape=[None, 3, None], dtype='int64')\n",
    "print('batched_x =', batched_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f92d57",
   "metadata": {},
   "source": [
    "### 2. 使用静态图组建网络\n",
    "在飞桨中，数据计算类API统一称为**Operator（算子）,简称OP**。  \n",
    "- 下面是一个完整的静态图计算网络的例子，这个例子中完成一个简单的“result = a + b”运算\n",
    "    - 定义了两个int64类型的输入数据a和b\n",
    "    - 并使用elementwise_add OP来对a、b进行“逐元素加和”的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b753659e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\lc-or\\lib\\site-packages\\paddle\\fluid\\executor.py:1279: UserWarning: There are no operators in the program to be executed. If you pass Program manually, please use fluid.program_guard to ensure the current Program is being used.\n",
      "  warnings.warn(error_info)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter an integer: a=1\n",
      "Please enter an integer: b=2\n",
      "---------------------------------\n",
      "1+2=3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*- \n",
    "\"\"\"\n",
    "Created on 2022/3/7 18:34\n",
    "\n",
    "使用静态图组建网络\n",
    "\n",
    "@Author : Jiabin Wang \n",
    "\"\"\"\n",
    "\n",
    "import paddle\n",
    "import numpy\n",
    "\n",
    "paddle.enable_static()\n",
    "a = paddle.static.data(name=\"a\", shape=[None, 1], dtype=\"int64\")\n",
    "b = paddle.static.data(name=\"b\", shape=[None, 1], dtype=\"int64\")\n",
    "\n",
    "# 组建网络，此处网络仅有一个操作构成\n",
    "result = paddle.add(a, b)\n",
    "\n",
    "place = paddle.CPUPlace()  # 定义运算设备\n",
    "exe = paddle.static.Executor(place=place)  # 创建执行器\n",
    "exe.run(paddle.static.default_startup_program())  # 网络参数初始化\n",
    "\n",
    "# 读取输入数据\n",
    "data_1 = int(input(\"Please enter an integer: a=\"))\n",
    "data_2 = int(input(\"Please enter an integer: b=\"))\n",
    "print('---------------------------------')\n",
    "x = numpy.array([[data_1]]).astype('int64')\n",
    "y = numpy.array([[data_2]]).astype('int64')\n",
    "\n",
    "# 运行网络\n",
    "outs = exe.run(\n",
    "    feed={'a': x, 'b': y},  # 给变量赋值\n",
    "    fetch_list=[result]  # 指定要获取的变量\n",
    ")\n",
    "\n",
    "# 输出计算结果\n",
    "print(\"%d+%d=%d\" % (data_1, data_2, outs[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf305f8b",
   "metadata": {},
   "source": [
    "- 在动态图中可以方便的用python的控制流语句来进行条件判断，但是在静态图中，在组网阶段没有实际执行操作，也没有产生中间计算结果，因此无法使用pyhton控制流语句进行条件判断。  \n",
    "- 为此静态图提供了多个控制流OP来实现条件判断语句。这里以`paddle.static.nn.while_loop`为例来说明如何在静态图中实现条件循环的操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b867e9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([10], dtype=int64)]\n"
     ]
    }
   ],
   "source": [
    "# 该示例代码展示整数循环+1，循环10次，输出计数结果\n",
    "import paddle\n",
    "\n",
    "# 定义cond方法，作为while_loop的判断条件\n",
    "def cond(i, ten):\n",
    "    return i < ten\n",
    "\n",
    "\n",
    "# 定义body方法，作为while_loop的执行体，只要cond返回值为True，while_loop就会一直调用该方法进行计算\n",
    "# 由于在使用while_loop OP时，cond和body的参数都是由while_loop的loop_vars参数指定的，\n",
    "# 所以cond和body必须有相同数量的参数列表，因此body中虽然只需要i这个参数，\n",
    "# 但是仍然要保持参数列表个数为2，此处添加了一个dummy参数来进行\"占位\"\n",
    "def body(i, dummy):\n",
    "    # 计算过程是对输入参数i进行自增操作，即 i = i + 1\n",
    "    i = i + 1\n",
    "    return i, dummy\n",
    "\n",
    "# 为了与上面cell的代码进行隔离\n",
    "main_program = paddle.static.Program()\n",
    "startup_program = paddle.static.Program()\n",
    "with paddle.static.program_guard(main_program, startup_program):\n",
    "    paddle.enable_static()\n",
    "    i = paddle.full(shape=[1], fill_value=0, dtype='int64')  # 循环计数器\n",
    "    ten = paddle.full(shape=[1], fill_value=10, dtype='int64')  # 循环次数\n",
    "    # while_loop的返回值是一个tensor列表，其长度，结构，类型与loop_vars相同\n",
    "    out, ten = paddle.static.nn.while_loop(cond, body, [i, ten])\n",
    "\n",
    "    exe = paddle.static.Executor(place=paddle.CPUPlace())\n",
    "    res = exe.run(paddle.static.default_main_program(), feed={}, fetch_list=out)\n",
    "    print(res)  # [array([10], dtype=int64)]\n",
    "paddle.disable_static()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd71131a",
   "metadata": {},
   "source": [
    "### 3. 根据动态图写静态图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3cc4eefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item  3/12 [=====>........................] - ETA: 0s - 6ms/item "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:40: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:40: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "Cache file C:\\Users\\wangjiabin01\\.cache\\paddle\\dataset\\uci_housing\\housing.data not found, downloading http://paddlemodels.bj.bcebos.com/uci_housing/housing.data \n",
      "Begin to download\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item 12/12 [==========================>...] - ETA: 0s - 4ms/itemepoch: 0, batch_id: 10, loss is: [572.2786]\n",
      "epoch: 0, batch_id: 20, loss is: [379.65326]\n",
      "epoch: 1, batch_id: 10, loss is: [494.0562]\n",
      "epoch: 1, batch_id: 20, loss is: [553.76495]\n",
      "epoch: 2, batch_id: 10, loss is: [661.4332]\n",
      "epoch: 2, batch_id: 20, loss is: [415.32324]\n",
      "epoch: 3, batch_id: 10, loss is: [441.50348]\n",
      "epoch: 3, batch_id: 20, loss is: [430.46228]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Download finished\n",
      "C:\\Users\\wangjiabin01\\AppData\\Local\\Temp/ipykernel_20668/435109424.py:40: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if batch_id % 10 == 0 and batch_id is not 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 4, batch_id: 10, loss is: [373.88834]\n",
      "epoch: 4, batch_id: 20, loss is: [240.96436]\n",
      "epoch: 5, batch_id: 10, loss is: [391.0603]\n",
      "epoch: 5, batch_id: 20, loss is: [273.99362]\n",
      "epoch: 6, batch_id: 10, loss is: [372.81314]\n",
      "epoch: 6, batch_id: 20, loss is: [610.01404]\n",
      "epoch: 7, batch_id: 10, loss is: [318.67462]\n",
      "epoch: 7, batch_id: 20, loss is: [238.03229]\n",
      "epoch: 8, batch_id: 10, loss is: [552.6242]\n",
      "epoch: 8, batch_id: 20, loss is: [567.91034]\n",
      "epoch: 9, batch_id: 10, loss is: [331.6791]\n",
      "epoch: 9, batch_id: 20, loss is: [138.4335]\n",
      "epoch: 10, batch_id: 10, loss is: [385.5222]\n",
      "epoch: 10, batch_id: 20, loss is: [266.11]\n",
      "epoch: 11, batch_id: 10, loss is: [310.43417]\n",
      "epoch: 11, batch_id: 20, loss is: [72.60823]\n",
      "epoch: 12, batch_id: 10, loss is: [468.53107]\n",
      "epoch: 12, batch_id: 20, loss is: [122.37219]\n",
      "epoch: 13, batch_id: 10, loss is: [246.44864]\n",
      "epoch: 13, batch_id: 20, loss is: [158.34715]\n",
      "epoch: 14, batch_id: 10, loss is: [178.89157]\n",
      "epoch: 14, batch_id: 20, loss is: [498.45743]\n",
      "epoch: 15, batch_id: 10, loss is: [132.84955]\n",
      "epoch: 15, batch_id: 20, loss is: [401.0933]\n",
      "epoch: 16, batch_id: 10, loss is: [283.8029]\n",
      "epoch: 16, batch_id: 20, loss is: [63.484188]\n",
      "epoch: 17, batch_id: 10, loss is: [173.11777]\n",
      "epoch: 17, batch_id: 20, loss is: [150.49329]\n",
      "epoch: 18, batch_id: 10, loss is: [175.45724]\n",
      "epoch: 18, batch_id: 20, loss is: [109.53139]\n",
      "epoch: 19, batch_id: 10, loss is: [164.99931]\n",
      "epoch: 19, batch_id: 20, loss is: [59.76731]\n",
      "epoch: 20, batch_id: 10, loss is: [147.01883]\n",
      "epoch: 20, batch_id: 20, loss is: [92.6905]\n",
      "epoch: 21, batch_id: 10, loss is: [134.51111]\n",
      "epoch: 21, batch_id: 20, loss is: [48.894264]\n",
      "epoch: 22, batch_id: 10, loss is: [149.19511]\n",
      "epoch: 22, batch_id: 20, loss is: [90.2132]\n",
      "epoch: 23, batch_id: 10, loss is: [146.19441]\n",
      "epoch: 23, batch_id: 20, loss is: [49.406128]\n",
      "epoch: 24, batch_id: 10, loss is: [152.36986]\n",
      "epoch: 24, batch_id: 20, loss is: [428.63843]\n",
      "epoch: 25, batch_id: 10, loss is: [198.70377]\n",
      "epoch: 25, batch_id: 20, loss is: [243.47546]\n",
      "epoch: 26, batch_id: 10, loss is: [106.24029]\n",
      "epoch: 26, batch_id: 20, loss is: [92.12117]\n",
      "epoch: 27, batch_id: 10, loss is: [91.14369]\n",
      "epoch: 27, batch_id: 20, loss is: [30.328938]\n",
      "epoch: 28, batch_id: 10, loss is: [96.39966]\n",
      "epoch: 28, batch_id: 20, loss is: [63.037262]\n",
      "epoch: 29, batch_id: 10, loss is: [126.30649]\n",
      "epoch: 29, batch_id: 20, loss is: [107.265495]\n",
      "epoch: 30, batch_id: 10, loss is: [79.91392]\n",
      "epoch: 30, batch_id: 20, loss is: [79.037704]\n",
      "epoch: 31, batch_id: 10, loss is: [20.619883]\n",
      "epoch: 31, batch_id: 20, loss is: [120.63702]\n",
      "epoch: 32, batch_id: 10, loss is: [65.38158]\n",
      "epoch: 32, batch_id: 20, loss is: [156.56277]\n",
      "epoch: 33, batch_id: 10, loss is: [121.983185]\n",
      "epoch: 33, batch_id: 20, loss is: [40.49076]\n",
      "epoch: 34, batch_id: 10, loss is: [51.523582]\n",
      "epoch: 34, batch_id: 20, loss is: [65.2529]\n",
      "epoch: 35, batch_id: 10, loss is: [76.113655]\n",
      "epoch: 35, batch_id: 20, loss is: [14.352503]\n",
      "epoch: 36, batch_id: 10, loss is: [49.342102]\n",
      "epoch: 36, batch_id: 20, loss is: [57.064854]\n",
      "epoch: 37, batch_id: 10, loss is: [158.8833]\n",
      "epoch: 37, batch_id: 20, loss is: [12.581133]\n",
      "epoch: 38, batch_id: 10, loss is: [18.820107]\n",
      "epoch: 38, batch_id: 20, loss is: [5.1625676]\n",
      "epoch: 39, batch_id: 10, loss is: [103.60336]\n",
      "epoch: 39, batch_id: 20, loss is: [9.157136]\n",
      "epoch: 40, batch_id: 10, loss is: [30.564848]\n",
      "epoch: 40, batch_id: 20, loss is: [15.4217005]\n",
      "epoch: 41, batch_id: 10, loss is: [26.20308]\n",
      "epoch: 41, batch_id: 20, loss is: [312.24255]\n",
      "epoch: 42, batch_id: 10, loss is: [29.156471]\n",
      "epoch: 42, batch_id: 20, loss is: [39.581036]\n",
      "epoch: 43, batch_id: 10, loss is: [136.7985]\n",
      "epoch: 43, batch_id: 20, loss is: [50.38967]\n",
      "epoch: 44, batch_id: 10, loss is: [173.05447]\n",
      "epoch: 44, batch_id: 20, loss is: [147.41011]\n",
      "epoch: 45, batch_id: 10, loss is: [91.94238]\n",
      "epoch: 45, batch_id: 20, loss is: [3.65198]\n",
      "epoch: 46, batch_id: 10, loss is: [124.83549]\n",
      "epoch: 46, batch_id: 20, loss is: [92.32531]\n",
      "epoch: 47, batch_id: 10, loss is: [23.924126]\n",
      "epoch: 47, batch_id: 20, loss is: [43.42848]\n",
      "epoch: 48, batch_id: 10, loss is: [146.89026]\n",
      "epoch: 48, batch_id: 20, loss is: [94.38567]\n",
      "epoch: 49, batch_id: 10, loss is: [74.61176]\n",
      "epoch: 49, batch_id: 20, loss is: [15.064471]\n",
      "epoch: 50, batch_id: 10, loss is: [21.032055]\n",
      "epoch: 50, batch_id: 20, loss is: [81.049614]\n",
      "epoch: 51, batch_id: 10, loss is: [110.91775]\n",
      "epoch: 51, batch_id: 20, loss is: [10.067332]\n",
      "epoch: 52, batch_id: 10, loss is: [61.4022]\n",
      "epoch: 52, batch_id: 20, loss is: [23.652294]\n",
      "epoch: 53, batch_id: 10, loss is: [13.591518]\n",
      "epoch: 53, batch_id: 20, loss is: [90.44563]\n",
      "epoch: 54, batch_id: 10, loss is: [108.56559]\n",
      "epoch: 54, batch_id: 20, loss is: [8.465523]\n",
      "epoch: 55, batch_id: 10, loss is: [89.455345]\n",
      "epoch: 55, batch_id: 20, loss is: [12.236795]\n",
      "epoch: 56, batch_id: 10, loss is: [139.10243]\n",
      "epoch: 56, batch_id: 20, loss is: [7.170667]\n",
      "epoch: 57, batch_id: 10, loss is: [81.15183]\n",
      "epoch: 57, batch_id: 20, loss is: [22.045008]\n",
      "epoch: 58, batch_id: 10, loss is: [110.69341]\n",
      "epoch: 58, batch_id: 20, loss is: [124.277855]\n",
      "epoch: 59, batch_id: 10, loss is: [119.94913]\n",
      "epoch: 59, batch_id: 20, loss is: [5.4075007]\n",
      "epoch: 60, batch_id: 10, loss is: [102.41256]\n",
      "epoch: 60, batch_id: 20, loss is: [19.972536]\n",
      "epoch: 61, batch_id: 10, loss is: [20.461096]\n",
      "epoch: 61, batch_id: 20, loss is: [72.13479]\n",
      "epoch: 62, batch_id: 10, loss is: [43.601723]\n",
      "epoch: 62, batch_id: 20, loss is: [89.90872]\n",
      "epoch: 63, batch_id: 10, loss is: [44.62137]\n",
      "epoch: 63, batch_id: 20, loss is: [20.681986]\n",
      "epoch: 64, batch_id: 10, loss is: [57.398098]\n",
      "epoch: 64, batch_id: 20, loss is: [3.7804928]\n",
      "epoch: 65, batch_id: 10, loss is: [27.79579]\n",
      "epoch: 65, batch_id: 20, loss is: [148.64722]\n",
      "epoch: 66, batch_id: 10, loss is: [106.37407]\n",
      "epoch: 66, batch_id: 20, loss is: [416.42993]\n",
      "epoch: 67, batch_id: 10, loss is: [33.32184]\n",
      "epoch: 67, batch_id: 20, loss is: [141.23743]\n",
      "epoch: 68, batch_id: 10, loss is: [87.479355]\n",
      "epoch: 68, batch_id: 20, loss is: [37.604492]\n",
      "epoch: 69, batch_id: 10, loss is: [20.17447]\n",
      "epoch: 69, batch_id: 20, loss is: [259.58554]\n",
      "epoch: 70, batch_id: 10, loss is: [53.678875]\n",
      "epoch: 70, batch_id: 20, loss is: [22.20752]\n",
      "epoch: 71, batch_id: 10, loss is: [17.458864]\n",
      "epoch: 71, batch_id: 20, loss is: [195.4123]\n",
      "epoch: 72, batch_id: 10, loss is: [23.853262]\n",
      "epoch: 72, batch_id: 20, loss is: [12.934563]\n",
      "epoch: 73, batch_id: 10, loss is: [40.139477]\n",
      "epoch: 73, batch_id: 20, loss is: [24.527596]\n",
      "epoch: 74, batch_id: 10, loss is: [54.336823]\n",
      "epoch: 74, batch_id: 20, loss is: [3.6599693]\n",
      "epoch: 75, batch_id: 10, loss is: [79.59507]\n",
      "epoch: 75, batch_id: 20, loss is: [16.187586]\n",
      "epoch: 76, batch_id: 10, loss is: [56.53946]\n",
      "epoch: 76, batch_id: 20, loss is: [28.992863]\n",
      "epoch: 77, batch_id: 10, loss is: [124.61044]\n",
      "epoch: 77, batch_id: 20, loss is: [28.149622]\n",
      "epoch: 78, batch_id: 10, loss is: [61.449814]\n",
      "epoch: 78, batch_id: 20, loss is: [101.36128]\n",
      "epoch: 79, batch_id: 10, loss is: [62.176937]\n",
      "epoch: 79, batch_id: 20, loss is: [8.081237]\n",
      "epoch: 80, batch_id: 10, loss is: [91.877075]\n",
      "epoch: 80, batch_id: 20, loss is: [181.89658]\n",
      "epoch: 81, batch_id: 10, loss is: [36.542328]\n",
      "epoch: 81, batch_id: 20, loss is: [29.925259]\n",
      "epoch: 82, batch_id: 10, loss is: [41.11337]\n",
      "epoch: 82, batch_id: 20, loss is: [17.356434]\n",
      "epoch: 83, batch_id: 10, loss is: [134.73969]\n",
      "epoch: 83, batch_id: 20, loss is: [10.533416]\n",
      "epoch: 84, batch_id: 10, loss is: [89.39771]\n",
      "epoch: 84, batch_id: 20, loss is: [9.88028]\n",
      "epoch: 85, batch_id: 10, loss is: [22.925259]\n",
      "epoch: 85, batch_id: 20, loss is: [26.994509]\n",
      "epoch: 86, batch_id: 10, loss is: [28.681152]\n",
      "epoch: 86, batch_id: 20, loss is: [25.22791]\n",
      "epoch: 87, batch_id: 10, loss is: [110.34074]\n",
      "epoch: 87, batch_id: 20, loss is: [25.799526]\n",
      "epoch: 88, batch_id: 10, loss is: [100.760086]\n",
      "epoch: 88, batch_id: 20, loss is: [11.714304]\n",
      "epoch: 89, batch_id: 10, loss is: [45.321575]\n",
      "epoch: 89, batch_id: 20, loss is: [127.39881]\n",
      "epoch: 90, batch_id: 10, loss is: [53.144096]\n",
      "epoch: 90, batch_id: 20, loss is: [130.66953]\n",
      "epoch: 91, batch_id: 10, loss is: [86.039246]\n",
      "epoch: 91, batch_id: 20, loss is: [6.3440933]\n",
      "epoch: 92, batch_id: 10, loss is: [106.27577]\n",
      "epoch: 92, batch_id: 20, loss is: [12.295018]\n",
      "epoch: 93, batch_id: 10, loss is: [59.91873]\n",
      "epoch: 93, batch_id: 20, loss is: [19.645866]\n",
      "epoch: 94, batch_id: 10, loss is: [43.65963]\n",
      "epoch: 94, batch_id: 20, loss is: [289.53192]\n",
      "epoch: 95, batch_id: 10, loss is: [43.291523]\n",
      "epoch: 95, batch_id: 20, loss is: [13.762261]\n",
      "epoch: 96, batch_id: 10, loss is: [54.04129]\n",
      "epoch: 96, batch_id: 20, loss is: [8.094105]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 97, batch_id: 10, loss is: [11.765215]\n",
      "epoch: 97, batch_id: 20, loss is: [89.3209]\n",
      "epoch: 98, batch_id: 10, loss is: [92.6944]\n",
      "epoch: 98, batch_id: 20, loss is: [14.764287]\n",
      "epoch: 99, batch_id: 10, loss is: [31.108477]\n",
      "epoch: 99, batch_id: 20, loss is: [235.86124]\n",
      "checkpoint loaded.\n",
      "[[14.106716]\n",
      " [14.193714]\n",
      " [13.857725]\n",
      " [15.989067]\n",
      " [14.674107]\n",
      " [15.668662]\n",
      " [15.171934]\n",
      " [15.025968]\n",
      " [12.281435]\n",
      " [14.525974]\n",
      " [11.457995]\n",
      " [13.711352]\n",
      " [14.547316]\n",
      " [13.685104]\n",
      " [13.65205 ]\n",
      " [15.145768]\n",
      " [15.960554]\n",
      " [15.793779]\n",
      " [16.109455]\n",
      " [14.739093]]\n",
      "id: prediction ground_truth\n",
      "0: 14.11 8.50\n",
      "1: 14.19 5.00\n",
      "2: 13.86 11.90\n",
      "3: 15.99 27.90\n",
      "4: 14.67 17.20\n",
      "5: 15.67 27.50\n",
      "6: 15.17 15.00\n",
      "7: 15.03 17.20\n",
      "8: 12.28 17.90\n",
      "9: 14.53 16.30\n",
      "10: 11.46 7.00\n",
      "11: 13.71 7.20\n",
      "12: 14.55 7.50\n",
      "13: 13.69 10.40\n",
      "14: 13.65 8.80\n",
      "15: 15.15 8.40\n",
      "16: 15.96 16.70\n",
      "17: 15.79 14.20\n",
      "18: 16.11 20.80\n",
      "19: 14.74 13.40\n",
      "checkpoint loaded.\n",
      "[[14.106716]\n",
      " [14.193714]\n",
      " [13.857725]\n",
      " [15.989067]\n",
      " [14.674107]\n",
      " [15.668662]\n",
      " [15.171934]\n",
      " [15.025968]\n",
      " [12.281435]\n",
      " [14.525974]\n",
      " [11.457995]\n",
      " [13.711352]\n",
      " [14.547316]\n",
      " [13.685104]\n",
      " [13.65205 ]\n",
      " [15.145768]\n",
      " [15.960554]\n",
      " [15.793779]\n",
      " [16.109455]\n",
      " [14.739093]]\n",
      "id: prediction ground_truth\n",
      "0: 14.11 8.50\n",
      "1: 14.19 5.00\n",
      "2: 13.86 11.90\n",
      "3: 15.99 27.90\n",
      "4: 14.67 17.20\n",
      "5: 15.67 27.50\n",
      "6: 15.17 15.00\n",
      "7: 15.03 17.20\n",
      "8: 12.28 17.90\n",
      "9: 14.53 16.30\n",
      "10: 11.46 7.00\n",
      "11: 13.71 7.20\n",
      "12: 14.55 7.50\n",
      "13: 13.69 10.40\n",
      "14: 13.65 8.80\n",
      "15: 15.15 8.40\n",
      "16: 15.96 16.70\n",
      "17: 15.79 14.20\n",
      "18: 16.11 20.80\n",
      "19: 14.74 13.40\n"
     ]
    }
   ],
   "source": [
    "# 动态图：预测波士顿房价\n",
    "import paddle\n",
    "\n",
    "# 定义网络结构，该任务中使用线性回归模型，网络由一个FC层构成\n",
    "class LinearRegression(paddle.nn.Layer):\n",
    "    def __init__(self, input_dim, hidden):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = paddle.nn.Linear(input_dim, hidden)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 训练和预测的数据读取处理，这部分的用法动态图和静态图是一致的\n",
    "batch_size = 20\n",
    "train_reader = paddle.io.DataLoader(paddle.text.datasets.UCIHousing(mode='train'), batch_size=batch_size, shuffle=True)\n",
    "test_reader = paddle.io.DataLoader(paddle.text.datasets.UCIHousing(mode='test'), batch_size=batch_size)\n",
    "\n",
    "# 波士顿房价预测任务中，共有13个特征\n",
    "input_feature = 13\n",
    "\n",
    "# 定义网络\n",
    "model = LinearRegression(input_dim=input_feature, hidden=1)\n",
    "# 定义优化器\n",
    "sgd = paddle.optimizer.SGD(learning_rate=0.001, parameters=model.parameters())\n",
    "\n",
    "max_epoch_num = 100  # 执行max_epoch_num次训练\n",
    "for epoch in range(max_epoch_num):\n",
    "    # 读取训练数据进行训练\n",
    "    for batch_id, data in enumerate(train_reader()):\n",
    "        x_tensor, y_tensor = data\n",
    "        # 调用网络，执行前向计算\n",
    "        prediction = model(x_tensor)\n",
    "\n",
    "        # 计算损失值\n",
    "        loss = paddle.nn.functional.square_error_cost(prediction, y_tensor)\n",
    "        avg_loss = paddle.mean(loss)\n",
    "\n",
    "        if batch_id % 10 == 0 and batch_id is not 0:\n",
    "            print(\"epoch: {}, batch_id: {}, loss is: {}\".format(epoch, batch_id, avg_loss.numpy()))\n",
    "\n",
    "        # 执行反向计算，并调用minimize接口计算和更新梯度\n",
    "        avg_loss.backward()\n",
    "        sgd.minimize(avg_loss)\n",
    "\n",
    "        # 将本次计算的梯度值清零，以便进行下一次迭代和梯度更新\n",
    "        model.clear_gradients()\n",
    "\n",
    "# 训练结束，保存训练好的模型\n",
    "paddle.save(model.state_dict(), 'linear.pdparams')\n",
    "\n",
    "linear_infer = LinearRegression(input_dim=input_feature, hidden=1)\n",
    "# 加载之前已经训练好的模型准备进行预测\n",
    "model_dict = paddle.load(\"linear.pdparams\")\n",
    "linear_infer.set_dict(model_dict)\n",
    "print(\"checkpoint loaded.\")\n",
    "\n",
    "# 开启评估测试模式（区别于训练模式）\n",
    "linear_infer.eval()\n",
    "(infer_x_tensor, infer_y_tensor) = next(test_reader())\n",
    "\n",
    "infer_result = linear_infer(infer_x_tensor)\n",
    "print(infer_result.numpy())\n",
    "\n",
    "print(\"id: prediction ground_truth\")\n",
    "for idx, val in enumerate(infer_result.numpy()):\n",
    "    print(\"%d: %.2f %.2f\" % (idx, val, infer_y_tensor.numpy()[idx]))\n",
    "    \n",
    "linear_infer = LinearRegression(input_dim=input_feature, hidden=1)\n",
    "# 加载之前已经训练好的模型准备进行预测\n",
    "model_dict = paddle.load(\"linear.pdparams\")\n",
    "linear_infer.set_dict(model_dict)\n",
    "print(\"checkpoint loaded.\")\n",
    "\n",
    "# 开启评估测试模式（区别于训练模式）\n",
    "linear_infer.eval()\n",
    "(infer_x_tensor, infer_y_tensor) = next(test_reader())\n",
    "\n",
    "infer_result = linear_infer(infer_x_tensor)\n",
    "print(infer_result.numpy())\n",
    "\n",
    "print(\"id: prediction ground_truth\")\n",
    "for idx, val in enumerate(infer_result.numpy()):\n",
    "    print(\"%d: %.2f %.2f\" % (idx, val, infer_y_tensor.numpy()[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c59002ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:55: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:55: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "C:\\Users\\wangjiabin01\\AppData\\Local\\Temp/ipykernel_20668/2678455673.py:55: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if batch_id % 10 == 0 and batch_id is not 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch_id: 10, loss is: [584.5659]\n",
      "epoch: 0, batch_id: 20, loss is: [576.61444]\n",
      "epoch: 1, batch_id: 10, loss is: [476.04672]\n",
      "epoch: 1, batch_id: 20, loss is: [408.51077]\n",
      "epoch: 2, batch_id: 10, loss is: [567.94775]\n",
      "epoch: 2, batch_id: 20, loss is: [573.7333]\n",
      "epoch: 3, batch_id: 10, loss is: [542.2305]\n",
      "epoch: 3, batch_id: 20, loss is: [438.43756]\n",
      "epoch: 4, batch_id: 10, loss is: [461.57367]\n",
      "epoch: 4, batch_id: 20, loss is: [924.50165]\n",
      "epoch: 5, batch_id: 10, loss is: [488.01862]\n",
      "epoch: 5, batch_id: 20, loss is: [298.5144]\n",
      "epoch: 6, batch_id: 10, loss is: [586.33655]\n",
      "epoch: 6, batch_id: 20, loss is: [205.19876]\n",
      "epoch: 7, batch_id: 10, loss is: [330.88754]\n",
      "epoch: 7, batch_id: 20, loss is: [579.54205]\n",
      "epoch: 8, batch_id: 10, loss is: [223.78882]\n",
      "epoch: 8, batch_id: 20, loss is: [285.00436]\n",
      "epoch: 9, batch_id: 10, loss is: [272.37854]\n",
      "epoch: 9, batch_id: 20, loss is: [218.97125]\n",
      "epoch: 10, batch_id: 10, loss is: [318.42395]\n",
      "epoch: 10, batch_id: 20, loss is: [240.31184]\n",
      "epoch: 11, batch_id: 10, loss is: [321.7784]\n",
      "epoch: 11, batch_id: 20, loss is: [68.04076]\n",
      "epoch: 12, batch_id: 10, loss is: [228.03156]\n",
      "epoch: 12, batch_id: 20, loss is: [354.27728]\n",
      "epoch: 13, batch_id: 10, loss is: [187.79947]\n",
      "epoch: 13, batch_id: 20, loss is: [366.18414]\n",
      "epoch: 14, batch_id: 10, loss is: [105.86009]\n",
      "epoch: 14, batch_id: 20, loss is: [90.70831]\n",
      "epoch: 15, batch_id: 10, loss is: [191.40585]\n",
      "epoch: 15, batch_id: 20, loss is: [136.2962]\n",
      "epoch: 16, batch_id: 10, loss is: [205.42076]\n",
      "epoch: 16, batch_id: 20, loss is: [248.73228]\n",
      "epoch: 17, batch_id: 10, loss is: [187.45155]\n",
      "epoch: 17, batch_id: 20, loss is: [67.963875]\n",
      "epoch: 18, batch_id: 10, loss is: [327.29736]\n",
      "epoch: 18, batch_id: 20, loss is: [236.74858]\n",
      "epoch: 19, batch_id: 10, loss is: [124.95746]\n",
      "epoch: 19, batch_id: 20, loss is: [443.30576]\n",
      "epoch: 20, batch_id: 10, loss is: [147.78125]\n",
      "epoch: 20, batch_id: 20, loss is: [177.16624]\n",
      "epoch: 21, batch_id: 10, loss is: [287.3672]\n",
      "epoch: 21, batch_id: 20, loss is: [113.74371]\n",
      "epoch: 22, batch_id: 10, loss is: [102.715935]\n",
      "epoch: 22, batch_id: 20, loss is: [22.67267]\n",
      "epoch: 23, batch_id: 10, loss is: [80.97953]\n",
      "epoch: 23, batch_id: 20, loss is: [22.44854]\n",
      "epoch: 24, batch_id: 10, loss is: [50.052414]\n",
      "epoch: 24, batch_id: 20, loss is: [166.15244]\n",
      "epoch: 25, batch_id: 10, loss is: [211.38246]\n",
      "epoch: 25, batch_id: 20, loss is: [153.83939]\n",
      "epoch: 26, batch_id: 10, loss is: [25.15824]\n",
      "epoch: 26, batch_id: 20, loss is: [20.957626]\n",
      "epoch: 27, batch_id: 10, loss is: [77.4557]\n",
      "epoch: 27, batch_id: 20, loss is: [267.41684]\n",
      "epoch: 28, batch_id: 10, loss is: [173.65062]\n",
      "epoch: 28, batch_id: 20, loss is: [30.960732]\n",
      "epoch: 29, batch_id: 10, loss is: [199.70966]\n",
      "epoch: 29, batch_id: 20, loss is: [23.77317]\n",
      "epoch: 30, batch_id: 10, loss is: [174.50044]\n",
      "epoch: 30, batch_id: 20, loss is: [6.9680166]\n",
      "epoch: 31, batch_id: 10, loss is: [126.41434]\n",
      "epoch: 31, batch_id: 20, loss is: [21.732473]\n",
      "epoch: 32, batch_id: 10, loss is: [81.12034]\n",
      "epoch: 32, batch_id: 20, loss is: [11.814969]\n",
      "epoch: 33, batch_id: 10, loss is: [77.50799]\n",
      "epoch: 33, batch_id: 20, loss is: [13.907329]\n",
      "epoch: 34, batch_id: 10, loss is: [50.037334]\n",
      "epoch: 34, batch_id: 20, loss is: [17.108711]\n",
      "epoch: 35, batch_id: 10, loss is: [111.847244]\n",
      "epoch: 35, batch_id: 20, loss is: [51.916695]\n",
      "epoch: 36, batch_id: 10, loss is: [130.13861]\n",
      "epoch: 36, batch_id: 20, loss is: [3.0243063]\n",
      "epoch: 37, batch_id: 10, loss is: [96.95772]\n",
      "epoch: 37, batch_id: 20, loss is: [53.385704]\n",
      "epoch: 38, batch_id: 10, loss is: [141.01613]\n",
      "epoch: 38, batch_id: 20, loss is: [26.84961]\n",
      "epoch: 39, batch_id: 10, loss is: [38.96742]\n",
      "epoch: 39, batch_id: 20, loss is: [323.34457]\n",
      "epoch: 40, batch_id: 10, loss is: [109.86846]\n",
      "epoch: 40, batch_id: 20, loss is: [51.342735]\n",
      "epoch: 41, batch_id: 10, loss is: [112.8768]\n",
      "epoch: 41, batch_id: 20, loss is: [36.634224]\n",
      "epoch: 42, batch_id: 10, loss is: [33.24387]\n",
      "epoch: 42, batch_id: 20, loss is: [19.491102]\n",
      "epoch: 43, batch_id: 10, loss is: [100.07251]\n",
      "epoch: 43, batch_id: 20, loss is: [75.72892]\n",
      "epoch: 44, batch_id: 10, loss is: [104.20514]\n",
      "epoch: 44, batch_id: 20, loss is: [10.831193]\n",
      "epoch: 45, batch_id: 10, loss is: [20.782293]\n",
      "epoch: 45, batch_id: 20, loss is: [167.8384]\n",
      "epoch: 46, batch_id: 10, loss is: [43.573658]\n",
      "epoch: 46, batch_id: 20, loss is: [9.055938]\n",
      "epoch: 47, batch_id: 10, loss is: [70.014915]\n",
      "epoch: 47, batch_id: 20, loss is: [5.197079]\n",
      "epoch: 48, batch_id: 10, loss is: [44.235435]\n",
      "epoch: 48, batch_id: 20, loss is: [3.788377]\n",
      "epoch: 49, batch_id: 10, loss is: [12.077011]\n",
      "epoch: 49, batch_id: 20, loss is: [8.18733]\n",
      "epoch: 50, batch_id: 10, loss is: [31.732616]\n",
      "epoch: 50, batch_id: 20, loss is: [21.078178]\n",
      "epoch: 51, batch_id: 10, loss is: [47.895832]\n",
      "epoch: 51, batch_id: 20, loss is: [170.86028]\n",
      "epoch: 52, batch_id: 10, loss is: [35.59359]\n",
      "epoch: 52, batch_id: 20, loss is: [13.246305]\n",
      "epoch: 53, batch_id: 10, loss is: [50.007725]\n",
      "epoch: 53, batch_id: 20, loss is: [20.933424]\n",
      "epoch: 54, batch_id: 10, loss is: [152.45222]\n",
      "epoch: 54, batch_id: 20, loss is: [24.315506]\n",
      "epoch: 55, batch_id: 10, loss is: [65.22122]\n",
      "epoch: 55, batch_id: 20, loss is: [21.43546]\n",
      "epoch: 56, batch_id: 10, loss is: [47.298653]\n",
      "epoch: 56, batch_id: 20, loss is: [124.319916]\n",
      "epoch: 57, batch_id: 10, loss is: [50.148838]\n",
      "epoch: 57, batch_id: 20, loss is: [102.70255]\n",
      "epoch: 58, batch_id: 10, loss is: [19.675587]\n",
      "epoch: 58, batch_id: 20, loss is: [40.47467]\n",
      "epoch: 59, batch_id: 10, loss is: [25.532276]\n",
      "epoch: 59, batch_id: 20, loss is: [12.714054]\n",
      "epoch: 60, batch_id: 10, loss is: [96.85755]\n",
      "epoch: 60, batch_id: 20, loss is: [1.5939457]\n",
      "epoch: 61, batch_id: 10, loss is: [89.29248]\n",
      "epoch: 61, batch_id: 20, loss is: [37.796955]\n",
      "epoch: 62, batch_id: 10, loss is: [81.8823]\n",
      "epoch: 62, batch_id: 20, loss is: [23.752356]\n",
      "epoch: 63, batch_id: 10, loss is: [82.14564]\n",
      "epoch: 63, batch_id: 20, loss is: [32.12613]\n",
      "epoch: 64, batch_id: 10, loss is: [76.94611]\n",
      "epoch: 64, batch_id: 20, loss is: [18.427135]\n",
      "epoch: 65, batch_id: 10, loss is: [63.82067]\n",
      "epoch: 65, batch_id: 20, loss is: [26.288464]\n",
      "epoch: 66, batch_id: 10, loss is: [78.98831]\n",
      "epoch: 66, batch_id: 20, loss is: [35.722237]\n",
      "epoch: 67, batch_id: 10, loss is: [24.340765]\n",
      "epoch: 67, batch_id: 20, loss is: [20.995659]\n",
      "epoch: 68, batch_id: 10, loss is: [53.47384]\n",
      "epoch: 68, batch_id: 20, loss is: [80.14789]\n",
      "epoch: 69, batch_id: 10, loss is: [35.22658]\n",
      "epoch: 69, batch_id: 20, loss is: [12.138863]\n",
      "epoch: 70, batch_id: 10, loss is: [29.840912]\n",
      "epoch: 70, batch_id: 20, loss is: [5.1508794]\n",
      "epoch: 71, batch_id: 10, loss is: [19.374699]\n",
      "epoch: 71, batch_id: 20, loss is: [13.582167]\n",
      "epoch: 72, batch_id: 10, loss is: [25.17297]\n",
      "epoch: 72, batch_id: 20, loss is: [30.257195]\n",
      "epoch: 73, batch_id: 10, loss is: [27.86299]\n",
      "epoch: 73, batch_id: 20, loss is: [16.67802]\n",
      "epoch: 74, batch_id: 10, loss is: [74.67001]\n",
      "epoch: 74, batch_id: 20, loss is: [72.2392]\n",
      "epoch: 75, batch_id: 10, loss is: [46.224087]\n",
      "epoch: 75, batch_id: 20, loss is: [101.94044]\n",
      "epoch: 76, batch_id: 10, loss is: [40.123005]\n",
      "epoch: 76, batch_id: 20, loss is: [50.418407]\n",
      "epoch: 77, batch_id: 10, loss is: [118.05165]\n",
      "epoch: 77, batch_id: 20, loss is: [19.708961]\n",
      "epoch: 78, batch_id: 10, loss is: [91.63987]\n",
      "epoch: 78, batch_id: 20, loss is: [22.432507]\n",
      "epoch: 79, batch_id: 10, loss is: [54.271805]\n",
      "epoch: 79, batch_id: 20, loss is: [10.514158]\n",
      "epoch: 80, batch_id: 10, loss is: [82.827065]\n",
      "epoch: 80, batch_id: 20, loss is: [68.87663]\n",
      "epoch: 81, batch_id: 10, loss is: [41.685635]\n",
      "epoch: 81, batch_id: 20, loss is: [19.763035]\n",
      "epoch: 82, batch_id: 10, loss is: [14.202473]\n",
      "epoch: 82, batch_id: 20, loss is: [153.49503]\n",
      "epoch: 83, batch_id: 10, loss is: [55.556713]\n",
      "epoch: 83, batch_id: 20, loss is: [24.993176]\n",
      "epoch: 84, batch_id: 10, loss is: [70.98555]\n",
      "epoch: 84, batch_id: 20, loss is: [25.79334]\n",
      "epoch: 85, batch_id: 10, loss is: [125.867744]\n",
      "epoch: 85, batch_id: 20, loss is: [92.12378]\n",
      "epoch: 86, batch_id: 10, loss is: [22.241434]\n",
      "epoch: 86, batch_id: 20, loss is: [22.153797]\n",
      "epoch: 87, batch_id: 10, loss is: [75.92491]\n",
      "epoch: 87, batch_id: 20, loss is: [27.045723]\n",
      "epoch: 88, batch_id: 10, loss is: [37.779346]\n",
      "epoch: 88, batch_id: 20, loss is: [100.18191]\n",
      "epoch: 89, batch_id: 10, loss is: [46.10195]\n",
      "epoch: 89, batch_id: 20, loss is: [19.93909]\n",
      "epoch: 90, batch_id: 10, loss is: [47.327198]\n",
      "epoch: 90, batch_id: 20, loss is: [12.286956]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 91, batch_id: 10, loss is: [27.64689]\n",
      "epoch: 91, batch_id: 20, loss is: [11.942961]\n",
      "epoch: 92, batch_id: 10, loss is: [128.92136]\n",
      "epoch: 92, batch_id: 20, loss is: [15.376593]\n",
      "epoch: 93, batch_id: 10, loss is: [61.570473]\n",
      "epoch: 93, batch_id: 20, loss is: [17.13982]\n",
      "epoch: 94, batch_id: 10, loss is: [62.24692]\n",
      "epoch: 94, batch_id: 20, loss is: [19.3348]\n",
      "epoch: 95, batch_id: 10, loss is: [95.80588]\n",
      "epoch: 95, batch_id: 20, loss is: [13.684561]\n",
      "epoch: 96, batch_id: 10, loss is: [21.51617]\n",
      "epoch: 96, batch_id: 20, loss is: [45.71259]\n",
      "epoch: 97, batch_id: 10, loss is: [20.027863]\n",
      "epoch: 97, batch_id: 20, loss is: [179.2323]\n",
      "epoch: 98, batch_id: 10, loss is: [63.03998]\n",
      "epoch: 98, batch_id: 20, loss is: [85.17096]\n",
      "epoch: 99, batch_id: 10, loss is: [59.8744]\n",
      "epoch: 99, batch_id: 20, loss is: [649.7136]\n",
      "id: prediction ground_truth\n",
      "0: 14.55 8.50\n",
      "1: 15.01 5.00\n",
      "2: 14.28 11.90\n",
      "3: 16.15 27.90\n",
      "4: 14.91 17.20\n",
      "5: 15.77 27.50\n",
      "6: 15.47 15.00\n",
      "7: 15.01 17.20\n",
      "8: 12.46 17.90\n",
      "9: 14.86 16.30\n",
      "10: 11.90 7.00\n",
      "11: 13.70 7.20\n",
      "12: 14.38 7.50\n",
      "13: 13.82 10.40\n",
      "14: 14.16 8.80\n",
      "15: 14.88 8.40\n",
      "16: 16.02 16.70\n",
      "17: 15.80 14.20\n",
      "18: 16.19 20.80\n",
      "19: 14.54 13.40\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "\n",
    "\n",
    "# 定义网络结构，该任务中使用线性回归模型，网络由一个fc层构成\n",
    "def linear_regression_net(input, hidden):\n",
    "    # 区别1：飞桨的大部分API可以做到动静通用，但也有部分带有明显特性的API只能用于动态图或静态图，在模型中需要按需使用，\n",
    "    # 另外由于动态图是基于面向对象的编程方式，而静态图还经常使用基于过程的编程方式，因此有些API虽然动静通用，\n",
    "    # 但在静态图中使用静态图专用API可能会更加方便，比如与paddle.nn.Linear对应的静态图API是paddle.static.nn.fc，\n",
    "    # 飞桨2.0及以上版本中，静态图专用API都统一置于paddle.static下\n",
    "    out = paddle.static.nn.fc(input, hidden)\n",
    "    return out\n",
    "\n",
    "\n",
    "# 区别2：飞桨2.0及以上版本默认动态图模式，因此需要显式开启静态图运行模式\n",
    "paddle.enable_static()\n",
    "# 区别3：在静态图中需要明确定义输入变量，即“占位符”，在静态图组网阶段并没有读入数据，所以需要使用占位符指明输入数据的类型、shape等信息\n",
    "# 波士顿房价预测任务中，共有13个特征，数据以batch形式组织，batch大小在定义时可以不确定，用None表示，因此shape=[None, 13]\n",
    "main_program = paddle.static.Program()\n",
    "startup_program = paddle.static.Program()\n",
    "with paddle.static.program_guard(main_program, startup_program):\n",
    "    x = paddle.static.data(name='x', shape=[None, 13], dtype='float32')\n",
    "    # y代表实际结果，只有一个值，因此shape=[None, 1]\n",
    "    y = paddle.static.data(name='y', shape=[None, 1], dtype='float32')\n",
    "\n",
    "    # 区别4：训练和预测的数据读取处理基本与动态图一致，但需要注意由于静态图在定义DataLoader时并没有实际读入数据，\n",
    "    # 所以需要通过feed_list参数指定要读取的变量列表\n",
    "    batch_size = 20\n",
    "    train_reader = paddle.io.DataLoader(paddle.text.datasets.UCIHousing(mode='train'), feed_list=[x, y],\n",
    "                                        batch_size=batch_size, shuffle=True)\n",
    "    test_reader = paddle.io.DataLoader(paddle.text.datasets.UCIHousing(mode='test'), feed_list=[x], batch_size=batch_size)\n",
    "\n",
    "    # 调用网络，执行前向计算\n",
    "    prediction = linear_regression_net(x, 1)\n",
    "\n",
    "    # 计算损失值\n",
    "    loss = paddle.nn.functional.square_error_cost(input=prediction, label=y)\n",
    "    avg_loss = paddle.mean(loss)\n",
    "\n",
    "    # 定义优化器，并调用minimize接口计算和更新梯度\n",
    "    sgd_optimizer = paddle.optimizer.SGD(learning_rate=0.001)\n",
    "    sgd_optimizer.minimize(avg_loss)\n",
    "\n",
    "    # 区别5：静态图中需要使用执行器执行之前已经定义好的网络，此处创建执行器\n",
    "    exe = paddle.static.Executor()\n",
    "\n",
    "    # 区别6：静态图中需要显式对网络进行初始化操作\n",
    "    exe.run(paddle.static.default_startup_program())\n",
    "\n",
    "    max_epoch_num = 100  # 执行max_epoch_num次训练\n",
    "    for epoch in range(max_epoch_num):\n",
    "        for batch_id, (x_tensor, y_tensor) in enumerate(train_reader()):\n",
    "            # 区别7：静态图中需要调用执行器的run方法执行计算过程，需要获取的计算结果（如avg_loss）需要通过fetch_list指定\n",
    "            avg_loss_value, = exe.run(feed={'x': x_tensor, 'y': y_tensor}, fetch_list=[avg_loss])\n",
    "\n",
    "            if batch_id % 10 == 0 and batch_id is not 0:\n",
    "                print(\"epoch: {}, batch_id: {}, loss is: {}\".format(epoch, batch_id, avg_loss_value))\n",
    "\n",
    "    # 区别8：静态图中需要使用save_inference_model来保存模型，以供预测使用\n",
    "    paddle.static.save_inference_model('./static_linear', [x], [prediction], exe)\n",
    "\n",
    "infer_exe = paddle.static.Executor()\n",
    "inference_scope = paddle.static.Scope()\n",
    "# 使用训练好的模型做预测\n",
    "with paddle.static.scope_guard(inference_scope):\n",
    "    # 区别9：静态图中需要使用load_inference_model来加载之前保存的模型\n",
    "    [inference_program, feed_target_names, fetch_targets\n",
    "     ] = paddle.static.load_inference_model('./static_linear', infer_exe)\n",
    "\n",
    "    # 读取一组测试数据\n",
    "    (infer_x, infer_y) = next(test_reader())\n",
    "\n",
    "    # 区别10：静态图中预测时也需要调用执行器的run方法执行计算过程，并指定之前加载的inference_program\n",
    "    results = infer_exe.run(\n",
    "        inference_program,\n",
    "        feed={feed_target_names[0]: infer_x},\n",
    "        fetch_list=fetch_targets)\n",
    "\n",
    "    print(\"id: prediction ground_truth\")\n",
    "    for idx, val in enumerate(results[0]):\n",
    "        print(\"%d: %.2f %.2f\" % (idx, val, infer_y.__array__()[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70b4d49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a85457",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
